# QuPath extension SAM

![](https://github.com/ksugar/samapi/releases/download/assets/qupath-samapi.gif)

This is a QuPath extension for [Segment Anything Model (SAM)](https://github.com/facebookresearch/segment-anything).

This is a part of the following paper. Please [cite](#citation) it when you use this project. You will also cite [the original SAM paper](https://arxiv.org/abs/2304.02643).

- Sugawara, K. [*Training deep learning models for cell image segmentation with sparse annotations.*](https://biorxiv.org/cgi/content/short/2023.06.13.544786v1) bioRxiv 2023. doi:10.1101/2023.06.13.544786

## Install

Drag and drop [the extension file](https://github.com/ksugar/qupath-extension-sam/releases/download/v0.1.0/qupath-extension-sam-0.1.0.jar) to [QuPath](https://qupath.github.io) and restart it.

Please note that you need to set up the server following the instructions in the link below.

[https://github.com/ksugar/samapi](https://github.com/ksugar/samapi)

## Usage

After opening an image on QuPath, select `Extensions` > `SAM` > `Enable SAM` > `ViT-H` (or `ViT-L` | `ViT-B`). See Figure 13 in [the SAM paper](https://arxiv.org/abs/2304.02643) for details about the image encoders.

![](https://github.com/ksugar/qupath-extension-sam/releases/download/assets/qupath-sam-enable.gif)

Once you select an image encoder, a selection made with the rectangle tool automatically generates a polygon annotation. The first run takes longer thant the following runs.

![](https://github.com/ksugar/qupath-extension-sam/releases/download/assets/qupath-sam-init.gif)

If you select a class in `Auto set` in the Annotations tab, it is used for a new annotation generated by SAM.

![](https://github.com/ksugar/qupath-extension-sam/releases/download/assets/qupath-extension-sam-class-auto-set.gif)

This command sends a request to http://localhost:8000 by default. The server URL can be set from `Extensions` > `SAM` > `Server URL`.

## Updates

- v0.2.0: Support any number of channels.

![](https://github.com/ksugar/qupath-extension-sam/releases/download/assets/qupath-sam-5channels-1080p.gif)

## Citation

Please cite my paper on [bioRxiv](https://biorxiv.org/cgi/content/short/2023.06.13.544786v1).

```.bib
@article {Sugawara2023.06.13.544786,
	author = {Ko Sugawara},
	title = {Training deep learning models for cell image segmentation with sparse annotations},
	elocation-id = {2023.06.13.544786},
	year = {2023},
	doi = {10.1101/2023.06.13.544786},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Deep learning is becoming more prominent in cell image analysis. However, collecting the annotated data required to train efficient deep-learning models remains a major obstacle. I demonstrate that functional performance can be achieved even with sparsely annotated data. Furthermore, I show that the selection of sparse cell annotations significantly impacts performance. I modified Cellpose and StarDist to enable training with sparsely annotated data and evaluated them in conjunction with ELEPHANT, a cell tracking algorithm that internally uses U-Net based cell segmentation. These results illustrate that sparse annotation is a generally effective strategy in deep learning-based cell image segmentation. Finally, I demonstrate that with the help of the Segment Anything Model (SAM), it is feasible to build an effective deep learning model of cell image segmentation from scratch just in a few minutes.Competing Interest StatementKS is employed part-time by LPIXEL Inc.},
	URL = {https://www.biorxiv.org/content/early/2023/06/13/2023.06.13.544786},
	eprint = {https://www.biorxiv.org/content/early/2023/06/13/2023.06.13.544786.full.pdf},
	journal = {bioRxiv}
}
```